# information-entropy
Information theory notes focused on entropy

### Thermodynamics Entropy
In thermodynamics, entropy can be described as the amount of disorder in a system, equivalent to the amount of energy unavaialable for work (*ΔG=ΔH−TΔS*). This concept of "disorder" is intuitive to a chemist, who understands that molecules tend to prefer states with greater degrees of conformational freedom. When designing a drug molecule, for example, the entropic cost of stabilizing the disordered sidechains of a protein binding site must be overcome by other favorable molecular interactions with the drug.

### Statistical Entropy
While the concept of molecular disorder helps to explain entropy in thermodynamics, it does not generalize well to understanding entropy at a quantitative level. A more quantitative description of entropy can be offered from a statistical perspective. Instead of "disorder", let's remember that entropy is simply a measure of the number of states (and their associated probabilities)
